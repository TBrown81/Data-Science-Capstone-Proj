# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkX2BTF2ZKjkVc3gXG0SgDam_8nDK_yJ

## Part 1

Importing necessary libraries
"""

import numpy as np # library to handle data in a vectorized manner

import pandas as pd # library for data analsysis
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

import json # library to handle JSON files

!conda install -c conda-forge geopy --yes
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values

import requests # library to handle requests
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe

# Matplotlib and associated plotting modules
import matplotlib.cm as cm
import matplotlib.colors as colors

# import k-means from clustering stage
from sklearn.cluster import KMeans

!conda install -c conda-forge folium=0.5.0 --yes 
import folium # map rendering library

print('Libraries imported.')

# scrape the web for data and add to new data frame
df=pd.read_html("https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M")[0]

#sort by postal code to append later data frame easier
df.sort_values(['Postal Code'])
df.head()

#drop 'not assigned' values
df = df[df.Borough != 'Not assigned']
df.head(11)

df.shape

"""The above code answers the first part of the assignment. I used the pandas read_html method to scrape the wiki page for the necessary data. 
I then added this data to a data frame, before dropping any rows containing "Not assigned" in the Borough collumn. I did this as these postal codes would be difficult to accurately cluster for the scope of this project otherwise. Herem we have actual location data to work with, via the borough and neighborhood names. The code also denotes the shape of the data frame as: 103,3.

## **Part 2**
"""

# scrape the web for data and add to new data frame
dfB=pd.read_html("https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M")[0]

#sort by postal code to append later data frame easier
dfB.sort_values(['Postal Code'])
dfB.head()

#gather the location data from the csv file. 
#add to a new dataframe
url='https://cocl.us/Geospatial_data'
dfLL=pd.read_csv(url)
dfLL.sort_values(['Postal Code'])
dfLL.head()

#ensure data imported correctly
dfB.loc[dfB['Postal Code']=='M5G']

#verify data imported correctly
dfLL.loc[dfLL['Postal Code']=='M5G']


#merge the dataframes on Postal Code to eliminate the problem in the above cell
df_union = pd.merge(dfLL, dfB, on = ['Postal Code'])
df_union.head()

#verify merge corrected the issue
df_union.loc[df_union['Postal Code'] == "M5G"]

#copy the above dataframe to fix the formatting
df_union_test = df_union
df_union_test.head()

#reformat the data to match the provided solution
df1 = df_union_test.pop('Latitude')
df2 = df_union_test.pop('Longitude')
df_union_test['Latitude'] = df1
df_union_test['Longitude'] = df2
df_union_test.head()

# find the index for the 12 required neighborhoods from the solution
df_union.loc[df_union['Postal Code'] == "M5A"]

#create new dataframe with required neighborhoods
df_answ = df_union_test.iloc[[57,17,35,5,38,43,11,101,96,68,0,53  ]]

#reset the index and display final answer
df_answ.reset_index(drop=True)

"""## **Part 3**"""

#get location data for Toronto
address = 'Toronto, Canada'

geolocator = Nominatim(user_agent="ca_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Toronto are {}, {}.'.format(latitude, longitude))

# create map of Toronto using latitude and longitude values
map_can = folium.Map(location=[latitude, longitude], zoom_start=10)

# add markers to map
for lat, lng, borough, Neighbourhood in zip(df_union_test['Latitude'], df_union_test['Longitude'], df_union_test['Borough'], df_union_test['Neighbourhood']):
    label = '{}, {}'.format(Neighbourhood, borough)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_can)  
    
map_can

#check how many boroughs and neighborhoods there are
print('The dataframe has {} boroughs and {} neighborhoods.'.format(
        len(df_union_test['Borough'].unique()),
        df_union_test.shape[0]
    )
)

#check how many unique values exist in the previous answer to make sure the above code is correct
df_union_test['Borough'].unique()

#create a new dataframe for the borugh i want to analyze: Downtown Toronto
dtt_data = df_union_test[df_union_test['Borough'] == 'Downtown Toronto'].reset_index(drop=True)
dtt_data.head()

#get geographic information for downtown toronto
address = 'Downtown Toronto,  Canada'

geolocator = Nominatim(user_agent="can_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Downtown Toronto are {}, {}.'.format(latitude, longitude))

# create map of Downtoan Toronto using latitude and longitude values
dtt_map = folium.Map(location=[latitude, longitude], zoom_start=11)

# add markers to map
for lat, lng, label in zip(dtt_data['Latitude'], dtt_data['Longitude'], dtt_data['Neighbourhood']):
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(dtt_map)  
    
dtt_map

#obligatory foursquare api setup
CLIENT_ID = 'R5VQOYFWT4MVIFFZFMUPBOYGAEOVHC0IZRXH2O5YDD4JB3L0' # your Foursquare ID
CLIENT_SECRET = 'ZGVMUMZSEUDBOUZUQT4TXFNCYUK0CGSRY5YM4PN02VGXCXQ3' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version
LIMIT = 100 # A default Foursquare API limit value

print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)

#looking at the first neighborhood in the borough
dtt_data.loc[0, 'Neighbourhood']

neighborhood_latitude = dtt_data.loc[0, 'Latitude'] # neighborhood latitude value
neighborhood_longitude = dtt_data.loc[0, 'Longitude'] # neighborhood longitude value

neighborhood_name = dtt_data.loc[0, 'Neighbourhood'] # neighborhood name

print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, 
                                                               neighborhood_latitude, 
                                                               neighborhood_longitude))

#generate URL for foursquare api call
LIMIT = 100
radius = 500

url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
    CLIENT_ID, 
    CLIENT_SECRET, 
    VERSION, 
    neighborhood_latitude, 
    neighborhood_longitude, 
    radius, 
    LIMIT)
url

#call foursquare api to get data
results = requests.get(url).json()
results

# function that extracts the category of the venue
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

venues = results['response']['groups'][0]['items']
    
nearby_venues = json_normalize(venues) # flatten JSON

# filter columns
filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']
nearby_venues =nearby_venues.loc[:, filtered_columns]

# filter the category for each row
nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)

# clean columns
nearby_venues.columns = [col.split(".")[-1] for col in nearby_venues.columns]

nearby_venues.head()

print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))

#perform above steps for all neighborhoods in Downtown Toronto
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)

# create new dataframe with all neighborhoods in Downtown Toronto
dtt_venues = getNearbyVenues(names=dtt_data['Neighbourhood'],
                                  latitudes=dtt_data['Latitude'],
                                  longitudes=dtt_data['Longitude']
                                  )

#confirm dataframe was created successfully
print(dtt_venues.shape)
dtt_venues.head()

dtt_venues.groupby('Neighborhood').count()

#determine number of unique categories
print('There are {} uniques categories.'.format(len(dtt_venues['Venue Category'].unique())))

# one hot encoding time
dtt_onehot = pd.get_dummies(dtt_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
dtt_onehot['Neighborhood'] = dtt_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [dtt_onehot.columns[-1]] + list(dtt_onehot.columns[:-1])
dttan_onehot = dtt_onehot[fixed_columns]

dtt_onehot.head()

dtt_onehot.shape

#find the mean frequency of the occurances in each category
dtt_grouped = dtt_onehot.groupby('Neighborhood').mean().reset_index()
dtt_grouped

dtt_grouped.shape

#print each neighborhoods top 5 venues. 
num_top_venues = 5

for hood in dtt_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = dtt_grouped[dtt_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')

#sort venues in descending order
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]

# create dataframe displaying top 10 venues in each nighborhood
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = dtt_grouped['Neighborhood']

for ind in np.arange(dtt_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(dtt_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()

#time to start the clustering
# set number of clusters
kclusters = 5

dtt_grouped_clustering = dtt_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(dtt_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10]



dtt_data.head()

neighborhoods_venues_sorted.head()

#dtt_data.rename(columns={"Neighbourhood": "x"})
#dtt_data.head()

# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

dtt_merged = dtt_data

# merge dtt_grouped with dtt_data to add latitude/longitude for each neighborhood
dtt_merged = dtt_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighbourhood')

dtt_merged.head() # check the last columns

# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(dtt_merged['Latitude'], dtt_merged['Longitude'], dtt_merged['Neighbourhood'], dtt_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters

#cluster 1
dtt_merged.loc[dtt_merged['Cluster Labels'] == 0,dtt_merged.columns[[2] + list(range(5, dtt_merged.shape[1]))]]

#cluster 2
dtt_merged.loc[dtt_merged['Cluster Labels'] == 1,dtt_merged.columns[[2] + list(range(5, dtt_merged.shape[1]))]]

#cluster 3
dtt_merged.loc[dtt_merged['Cluster Labels'] == 2,dtt_merged.columns[[2] + list(range(5, dtt_merged.shape[1]))]]

#cluster 4
dtt_merged.loc[dtt_merged['Cluster Labels'] == 3,dtt_merged.columns[[2] + list(range(5, dtt_merged.shape[1]))]]

#cluster 5
dtt_merged.loc[dtt_merged['Cluster Labels'] == 4,dtt_merged.columns[[2] + list(range(5, dtt_merged.shape[1]))]]

# Thank you for reading all this code!
# -Trevor
